{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e9a52b-f468-4cf3-abe8-256b4b46df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42edebea-d418-4d45-ba09-a92b8fb07a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440c4122-8a78-4284-9969-544fd9f7d701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsi/itaym/anaconda3/envs/maruloss/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ordinaloss.utils.pretrained_models\n",
      "loaded ordinaloss.utils.loss_utils\n",
      "loaded ordinaloss.utils.datasets\n"
     ]
    }
   ],
   "source": [
    "#from ordinaloss.utils.datasets import ImageFolder, create_transform_pipeline\n",
    "from ordinaloss.utils.pretrained_models import classification_model_densenet, classification_model_resnet, classification_model_vgg\n",
    "from ordinaloss.utils.utils import create_ordinal_cost_matrix\n",
    "\n",
    "\n",
    "from ordinaloss.engine.model_engine import OrdinalEngine\n",
    "from ordinaloss.utils.datasets import data_load\n",
    "\n",
    "from ordinaloss.utils.loss_utils import SinimLoss, GirlsLoss, CSCELoss\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4e846e2c-23fa-48c8-88fd-924869d48348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ordinaloss.utils.loss_utils\n"
     ]
    }
   ],
   "source": [
    "# train_transform = create_transform_pipeline(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3) #These are hyperparameters. \n",
    "# test_transform = create_transform_pipeline() #Without color jitter for test pipeline\n",
    "\n",
    "# train_dataset = ImageFolder(\"../datasets/kneeKL224/train/\", transform=train_transform)\n",
    "# val_dataset = ImageFolder(\"../datasets/kneeKL224/val/\", transform = test_transform)\n",
    "# test_dataset = ImageFolder(\"../datasets/kneeKL224/test/\", transform = test_transform)\n",
    "# auto_test_dataset = ImageFolder(\"../datasets/kneeKL224/auto_test/\", transform = test_transform)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True)\n",
    "# test_loader  = DataLoader(test_dataset,  batch_size = 128, shuffle=False)\n",
    "\n",
    "loaders, _, _ = data_load(\"../datasets/kneeKL224/\", 16)\n",
    "train_loader = loaders[\"train\"]\n",
    "test_loader = loaders[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1b1bd860-b117-4e8a-9ee8-0e4bd026cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = classification_model_vgg(\"vgg16\", num_classes = 5) #Could also be resnet34, resnet50, resnet101, resnet152, \n",
    "model = classification_model_vgg(\"vgg16\", num_classes = 5) #Could also be resnet34, resnet50, resnet101, resnet152, \n",
    "# model = classification_model_densenet(\"densenet121\", num_classes = 5) #Could also be densenet169, densenet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "094be05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20.,  1.,  2.,  3.,  4.],\n",
       "        [ 1., 20.,  1.,  2.,  3.],\n",
       "        [ 2.,  1., 20.,  1.,  2.],\n",
       "        [ 3.,  2.,  1., 20.,  1.],\n",
       "        [ 4.,  3.,  2.,  1., 20.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_ordinal_cost_matrix(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eb73c1be-b7ea-4cf5-8045-55f0bd9f4872",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 10.76 GiB total capacity; 5.11 GiB already allocated; 346.44 MiB free; 5.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39m0.9\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (epoch \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m5\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# engine = OrdinalEngine(model, \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#                        loss_fn = CSCELoss(create_ordinal_cost_matrix(5)), #CSCELoss model, matan, you know the dril.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#                        device = device, #change to gpu, let me know if it doesnt work on GPU.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#                        use_lr_scheduler=True, scheduler_lambda_fn = my_lambda_scheduler,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#                        optimizer_fn=SGD, lr = 5.0e-4, weight_decay=5.0e-4)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m engine \u001b[39m=\u001b[39m OrdinalEngine(model, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m                        loss_fn \u001b[39m=\u001b[39;49m CSCELoss(create_ordinal_cost_matrix(\u001b[39m5\u001b[39;49m)), \u001b[39m#CSCELoss model, matan, you know the dril.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m                        device \u001b[39m=\u001b[39;49m device, \u001b[39m#change to gpu, let me know if it doesnt work on GPU.\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m                        use_lr_scheduler\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, scheduler_lambda_fn \u001b[39m=\u001b[39;49m my_lambda_scheduler,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m                        optimizer_fn\u001b[39m=\u001b[39;49mAdam, lr \u001b[39m=\u001b[39;49m \u001b[39m1.0e-3\u001b[39;49m, weight_decay\u001b[39m=\u001b[39;49m\u001b[39m5.0e-2\u001b[39;49m)\n",
      "File \u001b[0;32m~/projects/ordinaloss/ordinaloss/engine/model_engine.py:46\u001b[0m, in \u001b[0;36mOrdinalEngine.__init__\u001b[0;34m(self, model, loss_fn, device, optimizer_fn, use_lr_scheduler, scheduler_lambda_fn, **optimizer_params)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m---> 46\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_optimizer(optimizer_fn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptimizer_params)\n\u001b[1;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_lr_scheduler \u001b[39m=\u001b[39m use_lr_scheduler\n",
      "File \u001b[0;32m~/anaconda3/envs/maruloss/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/maruloss/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/maruloss/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/maruloss/lib/python3.9/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/maruloss/lib/python3.9/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 10.76 GiB total capacity; 5.11 GiB already allocated; 346.44 MiB free; 5.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from ordinaloss.utils.loss_utils import GirlsLoss, SinimLoss\n",
    "model = classification_model_vgg(\"vgg16\", num_classes = 5)\n",
    "\n",
    "def my_lambda_scheduler(epoch):\n",
    "    return (0.9 ** (epoch // 5))\n",
    "\n",
    "\n",
    "# engine = OrdinalEngine(model, \n",
    "#                        loss_fn = CSCELoss(create_ordinal_cost_matrix(5)), #CSCELoss model, matan, you know the dril.\n",
    "#                        device = device, #change to gpu, let me know if it doesnt work on GPU.\n",
    "#                        use_lr_scheduler=True, scheduler_lambda_fn = my_lambda_scheduler,\n",
    "#                        optimizer_fn=SGD, lr = 5.0e-4, weight_decay=5.0e-4)\n",
    "\n",
    "\n",
    "# engine = OrdinalEngine(model, \n",
    "#                        loss_fn = SinimLoss(), #CSCELoss model, matan, you know the dril.\n",
    "#                        device = device, #change to gpu, let me know if it doesnt work on GPU.\n",
    "#                        use_lr_scheduler=True, scheduler_lambda_fn = my_lambda_scheduler,\n",
    "#                        optimizer_fn=SGD, lr = 5.0e-4, weight_decay=5.0e-4)\n",
    "\n",
    "\n",
    "engine = OrdinalEngine(model, \n",
    "                       loss_fn = CSCELoss(create_ordinal_cost_matrix(5)), #CSCELoss model, matan, you know the dril.\n",
    "                       device = device, #change to gpu, let me know if it doesnt work on GPU.\n",
    "                       use_lr_scheduler=True, scheduler_lambda_fn = my_lambda_scheduler,\n",
    "                       optimizer_fn=Adam, lr = 1.0e-3, weight_decay=5.0e-2)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "65833920",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "25d5f8a2-5fc4-4ef1-969b-52de90659805",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training, epoch 0:   0%|          | 0/362 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 10.76 GiB total capacity; 5.00 GiB already allocated; 348.44 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdsiaspl01/home/dsi/itaym/projects/ordinaloss/training_model_exmaple.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m engine\u001b[39m.\u001b[39;49mtrain(train_loader, test_loader, n_epochs \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/projects/ordinaloss/ordinaloss/engine/model_engine.py:203\u001b[0m, in \u001b[0;36mOrdinalEngine.train\u001b[0;34m(self, train_loader, test_loader, n_epochs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, train_loader, test_loader\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, n_epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    202\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m--> 203\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_loader)\n\u001b[1;32m    204\u001b[0m         \u001b[39mif\u001b[39;00m test_loader:\n\u001b[1;32m    205\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_epoch(test_loader))\n",
      "File \u001b[0;32m~/projects/ordinaloss/ordinaloss/engine/model_engine.py:114\u001b[0m, in \u001b[0;36mOrdinalEngine._train_epoch\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    111\u001b[0m cum_mae \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[39mfor\u001b[39;00m X, y \u001b[39min\u001b[39;00m iterator: \n\u001b[0;32m--> 114\u001b[0m     stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(X, y)\n\u001b[1;32m    115\u001b[0m     n \u001b[39m=\u001b[39m stats[\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    117\u001b[0m     cum_batch_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m n\n",
      "File \u001b[0;32m~/projects/ordinaloss/ordinaloss/engine/model_engine.py:101\u001b[0m, in \u001b[0;36mOrdinalEngine._train_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     99\u001b[0m stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(X, y)\n\u001b[0;32m--> 101\u001b[0m stats[\u001b[39m\"\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m stats\n",
      "File \u001b[0;32m~/anaconda3/envs/maruloss/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/maruloss/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 10.76 GiB total capacity; 5.00 GiB already allocated; 348.44 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "engine.train(train_loader, test_loader, n_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d089ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('maruloss')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e97c30b3269f9da24b9fb2b586032ecca136d76e1141d9f46df271ff014add6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
